{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9urQwGN0DrO",
    "tags": []
   },
   "source": [
    "# Notes for Readers / Markers\n",
    "\n",
    "This notebook is a combination of individual note books. We also did not remove or clean the code for presentation purposes. As such, the flow may not be totally coherant, and there may be examples of very poor or even nonsensical code snippets used for testing or checking for robustness / sanity checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for us:\n",
    "\n",
    "Paper source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5009332/\n",
    "\n",
    "Data source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5009332/bin/srep32586-s2.xls\n",
    "\n",
    "Paper for original analysis:\n",
    "\n",
    "https://osf.io/wzhe7/\n",
    "\n",
    "https://bmcplantbiol.biomedcentral.com/articles/10.1186/s12870-019-2039-9\n",
    "\n",
    "\n",
    "Goals for us:\n",
    "\n",
    "Compare normal regression to the quantile regression from the paper and critically analyse.\n",
    "\n",
    "Secondly, obviously these metrics will have correlations, and salt may affect 1 and only indirectly the other. For example, there may be a plausible connect between flowering time and ear number per plant. These may both be directly infleunced by an overall salt-tolerance factor, as well as each other. \n",
    "\n",
    "Structured equation modelling (SEM) allows us to model and test these correlations and define a general \"salt-tolerance\" factor. \n",
    "\n",
    "Theoretically, we can then use this factor to compare plant strains to see if any are more or less salt tolerant. Sample sizes we strain are relatively small, which is common in plant sciences, which allows for good critical discussion about the limitations of statistical inference. \n",
    "\n",
    "From here, if this is done in time, we can then include bayesian modelling to see if the observed data sufficiently updates our priors (control data + assumptions) to the point of significance. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cTv_KML0KtO"
   },
   "source": [
    "# Intro\n",
    "\n",
    "Why salt tolerance? Climate change, feeding people. Allows for testing plant genetics and development of salt resistance crops.\n",
    "\n",
    "Features: \n",
    "- Flowering time (HEA)\n",
    "- maturity time (MAT)\n",
    "- ripening period (RIP)\n",
    "- plant height (HEI)\n",
    "- thousand grain mass (TGW)\n",
    "- ear number per plant (EAR)\n",
    "- grain number per ear (GPE)\n",
    "- dry mass per m2 (DRY_WT)\n",
    "- yield (YLD)\n",
    "- harvest index (HI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNP0qUB30J4_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import statsmodels.api as sm\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "import semopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVjNxRqj1ISi"
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5009332/bin/srep32586-s2.xls\",skiprows=1)\n",
    "color = [\"Yellow\" if x == \"Control\" else \"Blue\" for x in data[\"Condition\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Z4pVOOlu1NmU",
    "outputId": "76f69597-1ab4-4d84-8352-2565634a220f"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CErAv0xpLqVv"
   },
   "source": [
    "#### General scatter matrix\n",
    "\n",
    "We see that on an overall level, correlations are relatively non-existant. However, once we break the data down into groups (Treatment, Year), the correlations become much more apparent.\n",
    "\n",
    "We exploit this structure to define an overall \"salt-tolerance\" factor. Finally we compare the correlation of the Salt tolerance factor to the Salt tolerance index used in the publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "IPetQFGTC3eN",
    "outputId": "bab13d59-fb77-4b03-dc2b-36f6e68392ff"
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(data, alpha=0.2, figsize=(20,10), c=color);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(data[data[\"Condition\"]==\"Control\"], alpha=0.2, figsize=(20,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(data[(data[\"Condition\"]==\"Control\") & (data[\"Year\"]==2014)], alpha=0.2, figsize=(20,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing.\n",
    "\n",
    "We want to model the response to the salt treatment, and see how each variable responds to the change. To do this, we essentially ask \"How much do you change with when we add salt\". To be able to compare with the (working, and used) SWP metric, and to produce a variable that is on the same scale as the original salt tolerance factor, we divide the treatment group by the square root of the control group.\n",
    "\n",
    "The dataframe below is the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data.groupby([\"Line\",\"Year\",\"Condition\"]).mean().reset_index()\n",
    "dt_c = dt[dt[\"Condition\"]== \"Control\"].loc[:,dt.columns != 'Condition'].groupby([\"Line\",\"Year\"]).mean()\n",
    "dt_t = dt[dt[\"Condition\"]!= \"Control\"].loc[:,dt.columns != 'Condition'].groupby([\"Line\",\"Year\"]).mean()\n",
    "\n",
    "# Center\n",
    "#dt_t = dt_t-dt_t.mean()\n",
    "#dt_c = dt_c-dt_c.mean()\n",
    "\n",
    "df = dt_t / np.sqrt(dt_c)\n",
    "dt_c.columns = [s[0:3].strip() for s in list(df.columns)]\n",
    "dt_t.columns = [s[0:3].strip() for s in list(df.columns)]\n",
    "df.columns = [s[0:3].strip() for s in list(df.columns)]\n",
    "\n",
    "ST_1 = dt_t[\"YLD\"] / np.sqrt(dt_c[\"YLD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df, alpha=0.2, figsize=(20,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data in for statsmodel, as single with all data is needed\n",
    "df['ST_1'] = ST_1  \n",
    "\n",
    "df1 = df.reset_index()\n",
    "\n",
    "#include year into dataset might be used\n",
    "df1['YEAR'] = [1 if x == 2014 else 0 for x in df1[\"Year\"]]\n",
    "\n",
    "df_train = df1.sample(frac = 0.67, random_state=1) #train data\n",
    "df_test = df1.drop(df_train.index) #test data (whole data - train data)\n",
    "\n",
    "y_test = df_test['ST_1'] \n",
    "X_test = df_test.drop(['ST_1'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B6KSBnKK1MN"
   },
   "source": [
    "### Run Linear regression and interpret\n",
    "\n",
    "We start off with a simple OLS regression to the mean. Regressing HEA + MAT + RIP + HEI  + EAR + GPE +HI +TGW +DRY + Year on ST_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK9BkaEMK0Wx"
   },
   "outputs": [],
   "source": [
    "lin_reg = smf.ols(formula='ST_1 ~ HEA + MAT + RIP + HEI  + EAR + GPE +HI +TGW +DRY',data =df_train).fit() #specify our model  and fit \n",
    "y_pred = lin_reg.predict(X_test) #make predictions\n",
    "\n",
    "mean_squared_error(y_test, y_pred) #calculate different performance meassures\n",
    "mean_absolute_error(y_test, y_pred)\n",
    "print(lin_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly our Regression suffers from multicollinearity. This is only a problem, if we want to interpret our coefficients, if the Regression for prediction this should be fine. Furthermore we should investigate if our model assumptions are meet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK9BkaEMK0Wx"
   },
   "outputs": [],
   "source": [
    "#calculate the residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# plot the histogram\n",
    "f = plt.figure()\n",
    "\n",
    "plt.hist(residuals,color='#c9e5de')\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.show()\n",
    "f.savefig('hist.png',bbox_inches='tight',dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly our Regression also suffers from heteroscedicity, rendering our predictions also useless. This will be further investigated in the R-code, with more beatiful plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK9BkaEMK0Wx"
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Observed Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deal with Mulitcollinarirty by dropping several columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK9BkaEMK0Wx"
   },
   "outputs": [],
   "source": [
    "lin_reg = smf.ols(formula='ST_1 ~ HEA + MAT + RIP + HEI  + EAR + GPE +HI',data =df_train).fit() #specify our model  and fit \n",
    "y_pred = lin_reg.predict(X_test) #make predictions\n",
    "mean_squared_error(y_test, y_pred) #calculate different performance meassures\n",
    "mean_absolute_error(y_test, y_pred)\n",
    "print(lin_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Regression\n",
    "#### Why is the Quantile Regression being used?\n",
    "The big advantage of a Quantile Regression over a 'normal' OLS Regression is that the Quantile Regression is more robust than the OLS Regression. Furthermore is the assumption of homoscedicity not nessasary for a quantile regression. As the quantile regression does not minimize the ordinary least square loss function such as the OLS regression, but the so called quantile loss, which is less affected by heteroskedasticity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_quant = []\n",
    "MAE_quant = []\n",
    "quantiles = [0.25,0.5,0.75]#different quantiles\n",
    "for i in quantiles:\n",
    "    mod = smf.quantreg(\"ST_1 ~ HEA + MAT + RIP + HEI + TGW + EAR + GPE + DRY + HI\", df_train) #specify our model\n",
    "    res = mod.fit(q=i) #fit model\n",
    "    y_pred = res.predict(X_test) #make predictions\n",
    "    MSE_quant.append(mean_squared_error(y_test, y_pred)) #calculate different performance meassures\n",
    "    MAE_quant.append(mean_absolute_error(y_test, y_pred))\n",
    "    print('Quantile Regression with q=',i,res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we face multicollinearity. This is makes sense as we used the same data and features. We can deal with this in similair fashion as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_quant = []\n",
    "MAE_quant = []\n",
    "quantiles = [0.25,0.5,0.75] #different quantiles\n",
    "for i in quantiles:\n",
    "    mod = smf.quantreg(\"ST_1 ~ HEA + MAT + RIP + HEI + EAR + GPE + HI\", df_train) #specify our model\n",
    "    res = mod.fit(q=i) #fit model\n",
    "    y_pred = res.predict(X_test) #make predictions\n",
    "    MSE_quant.append(mean_squared_error(y_test, y_pred)) #calculate different performance meassures\n",
    "    MAE_quant.append(mean_absolute_error(y_test, y_pred))\n",
    "    print('Quantile Regression with q=',i,res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next part:\n",
    "\n",
    "Above, we used the $\\frac{y_t}{sqrt(y_c)}$ as an index for salt tolerance (as suggested by the paper(s)).\n",
    "This index is obviously heavily dependent on yield, and is in essence a proxy for salt tolerance, it only considers the yield of the plant.\n",
    "\n",
    "Outside of food production, it can be beneficial if the plant grows faster, is thicker, produces more cholorophil and so forth. The dataset we have available to us, was curated with the purpose of testing salt tolerance. Despite the author's choice of salt tolerance index, there are still some general plant statistics that are not directly related to yield.\n",
    "\n",
    "We make some assumptions: primarily, that \"more is better\". Under these assumptions, is it possible to define a multi-dimensional measure of plant health and/or salt tolerance?\n",
    "\n",
    "We can model plant health based on the control plants. We can then apply this model to the treatment plants to get a measure for their plant health?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0auEIx__K0Lu"
   },
   "source": [
    "#### Run PCA/Other factor models and interpret the structure\n",
    "- How many factors do we have?\n",
    "- What is the factor structure?\n",
    "\n",
    "The below PCA suggests only a single factor. At most, maybe 2. This is in line with \"plant health\" and or \"salt tolerance\" being a relatively simple construct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['HEA', 'MAT', 'RIP', 'HEI', 'TGW', 'EAR', 'GPE', 'DRY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dt_t.loc[:,2013,:][cols]/np.sqrt(dt_t.loc[:,2013,:][cols]).values\n",
    "eigs, vecs = np.linalg.eig(np.cov(dat.T))\n",
    "plt.plot(range(1,9),sorted(eigs,reverse=True));\n",
    "plt.xlabel(\"Dimension\");\n",
    "plt.ylabel(\"Eigenvalue\");\n",
    "plt.title(\"Total variance explained\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkG-WnoTK0AS"
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(dt_t.loc[:,2013,:][cols]/np.sqrt(dt_t.loc[:,2013,:][cols]).values)\n",
    "plt.plot(pca.explained_variance_ratio_);\n",
    "plt.xlabel(\"test\")\n",
    "\n",
    "# Recheck the criteria of factor selection. Looks like we have at most 2 main factor.\n",
    "# Makes sense, as the dataset is supposed to measure Salt Tolerence.\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_data = dt_t.loc[:,2013,:][cols]/np.sqrt(dt_t.loc[:,2013,:][cols])\n",
    "fa = FactorAnalyzer(n_factors=1,rotation=\"varimax\")\n",
    "fa.fit(fit_data)\n",
    "df_loadings = pd.DataFrame(fa.loadings_, index=cols)\n",
    "df_loadings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_fa = fit_data @ df_loadings\n",
    "ST_fa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD-QAlWRKzbI"
   },
   "source": [
    "### Fit SEM model(s)\n",
    "- Are there subfactors?\n",
    "- Is there just one overall factor (in essence linear regression)\n",
    "\n",
    "\n",
    "\n",
    "##### Notes:\n",
    "\n",
    "- RIP fucks everything up. Looks like it has no relation to the rest.\n",
    "- Growth and Yield factors have no real relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flowering time (HEA), maturity time (MAT), ripening period (RIP), plant height (HEI), thousand grain mass (TGW), ear number per plant (EAR), grain number per ear (GPE), dry mass per m2 (DRY_WT), yield (YLD), and harvest index (HI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(dt_t.loc[:,2013,:]/np.sqrt(dt_c.loc[:,2013,:]), alpha=0.2, figsize=(20,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_c.loc[:,2013,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlqbxAMiKzPF"
   },
   "outputs": [],
   "source": [
    "# RIP is totally unrelated to everything apparently. Why? Idk\n",
    "\n",
    "desc_reg = \\\n",
    "'''\n",
    "Salt_Tol =~ EAR + GPE + DRY + TGW + HEA + MAT + HEI + RIP\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = semopy.Model(desc_reg)\n",
    "res = mod1.fit(dt_t.loc[:,2013,:]/np.sqrt(dt_c.loc[:,2013,:]), obj=\"ULS\") #Train on 2013 data\n",
    "print(res)\n",
    "print(mod1.inspect())\n",
    "\n",
    "# 1 Loading per factor is set to 1 for identification. \n",
    "# The other loadings are there in relative to this loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = mod1.inspect().iloc[0:8,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semopy.semplot(mod1, \"./model_uni.png\",std_ests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmsea: <0.05\n",
    "rmr: < 0.05\n",
    "cfi: >0.95\n",
    "tli: > 0.97\n",
    "aic/BIC smaller is better\n",
    "\n",
    "x2 / df < 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semopy.calc_stats(mod1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitdel = mod1.predict_factors(df.loc[:,2014,:]) # Fit to entire dataset\n",
    "\n",
    "  \n",
    "ST1 = dt_t.loc[:,2014,:][\"YLD\"]/np.sqrt(dt_c.loc[:,2014,:][\"YLD\"]) \n",
    "ST = fitdel.G\n",
    "#ST = (fitst.G/fitsc.ST).values\n",
    "plt.scatter(ST, ST1);\n",
    "np.corrcoef(ST, ST1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fitdel = mod1.predict_factors(df.loc[:,2013,:]) # Fit to entire dataset\n",
    "\n",
    "  \n",
    "ST1 = dt_t.loc[:,2013,:][\"YLD\"]/np.sqrt(dt_c.loc[:,2013,:][\"YLD\"]) \n",
    "ST = fitdel.G\n",
    "#ST = (fitst.G/fitsc.ST).values\n",
    "plt.scatter(ST, ST1);\n",
    "np.corrcoef(ST, ST1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ST_fa,ST);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ST_fa,ST1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = mod1.inspect().iloc[0:8,[0,3]].set_index(\"lval\")\n",
    "variances = mod1.inspect().iloc[9:,3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors.join(df_loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.outer(factors,factors));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.cov(df[order].T));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best fit so far\n",
    "desc1 = \\\n",
    "''' Salt.Tol =~ ST_Dev + ST_Prod\n",
    "    ST_Prod =~ EAR + GPE + DRY + TGW \n",
    "    ST_Dev =~ HEA + MAT + RIP + HEI\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKucDCEBKznp"
   },
   "outputs": [],
   "source": [
    "mod2 = semopy.Model(desc1)\n",
    "res = mod2.fit(dt_t.loc[:,2013,:]/np.sqrt(dt_c.loc[:,2013,:]), obj=\"ULS\", clean_slate=True) #Train on 2013 data\n",
    "print(res)\n",
    "print(mod2.inspect())\n",
    "\n",
    "# 1 Loading per factor is set to 1 for identification. \n",
    "# The other loadings are there in relative to this loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semopy.semplot(mod2, \"./model_bi.png\",plot_covs=True,std_ests=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semopy.calc_stats(mod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitdel = mod2.predict_factors(df.loc[:,2014,:]) # Fit to entire dataset\n",
    "\n",
    "  \n",
    "ST1 = dt_t.loc[:,2014,:][\"YLD\"]/np.sqrt(dt_c.loc[:,2014,:][\"YLD\"]) \n",
    "ST = fitdel.Yield\n",
    "#ST = (fitst.G/fitsc.ST).values\n",
    "plt.scatter(ST, ST1);\n",
    "np.corrcoef(ST, ST1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitdel = mod2.predict_factors(df.loc[:,2014,:]) # Fit to entire dataset\n",
    "\n",
    "  \n",
    "ST1 = dt_t.loc[:,2014,:][\"YLD\"]/np.sqrt(dt_c.loc[:,2014,:][\"YLD\"]) \n",
    "ST = fitdel.Growth\n",
    "#ST = (fitst.G/fitsc.ST).values\n",
    "plt.scatter(ST, ST1);\n",
    "np.corrcoef(ST, ST1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?semopy.semplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "desc2 = \\\n",
    "''' #ST =~ Growth + Yield \n",
    "    ST_Prod =~ EAR + GPE + DRY + TGW + HEI\n",
    "    ST_Dev =~ HEA + MAT + RIP + HEI + TGW\n",
    "    ST_Prod ~ ST_Dev\n",
    "\n",
    "    # Covariances - Something else correlating these, not explained by the model...\n",
    "    GPE ~~ 0 * GPE\n",
    "    TGW ~~ HEI\n",
    "'''\n",
    "\n",
    "mod3 = semopy.Model(desc2)\n",
    "res = mod3.fit(dt_t.loc[:,2013,:]/np.sqrt(dt_c.loc[:,2013,:]), obj=\"ULS\") #Train on 2013 data\n",
    "print(res)\n",
    "display(mod3.inspect(std_est=True))\n",
    "\n",
    "# 1 Loading per factor is set to 1 for identification. \n",
    "# The other loadings are there in relative to this loading.\n",
    "display(semopy.semplot(mod3, \"./model_sub.png\",std_ests=True))\n",
    "display(semopy.calc_stats(mod3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitdel = mod3.predict_factors(df.loc[:,2014,:]) # Fit to 2014 subset\n",
    "fitdel\n",
    "  \n",
    "#ST1 = dt_t.loc[:,2014,:][\"YLD\"]/np.sqrt(dt_c.loc[:,2014,:][\"YLD\"]) \n",
    "#ST = fitdel.ST_Yield\n",
    "\n",
    "#fig, ax = plt.subplots(2,2, figsize=(11,11))\n",
    "#ax[0,0].scatter(fitdel.ST_Yield, ST1, s=4, alpha=0.5);\n",
    "#ax[0,1].scatter(fitdel.ST_Growth, ST1, s=4, alpha=0.5);\n",
    "#ax[1,0].scatter(fitdel.ST_Growth, fitdel.ST_Yield, s=4, alpha=0.5);\n",
    "#np.corrcoef(ST, ST1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to make the model predict factors... Running into pure linear algebra problems. \n",
    "Semopy is a python port of Lavaan, and is not as robustly implemented (yet).\n",
    "\n",
    "At this point, we move over to R and test the models there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = mod3.vars['latent']\n",
    "num_lat = len(lats)\n",
    "x = df.loc[:,2014,:][mod3.vars['observed']].values.T\n",
    "lambda_h = mod3.mx_lambda[:,:num_lat]\n",
    "lambda_x = mod3.mx_lambda[:,num_lat:]\n",
    "c = np.linalg.inv(np.identity(mod3.mx_beta.shape[0]) - mod3.mx_beta)\n",
    "c_1 = c[:num_lat, :]\n",
    "c_2 = c[num_lat:, :]\n",
    "M_h = x\n",
    "t = lambda_x @ c_2\n",
    "L_zh = (t @ mod3.mx_psi @ t.T + mod3.mx_theta) * (x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
